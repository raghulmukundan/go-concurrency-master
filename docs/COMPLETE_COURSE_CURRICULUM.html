<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Go Concurrency Mastery: Complete Course Curriculum - Go Concurrency Course</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,500;8..60,600;8..60,700&family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" class="hljs-theme" data-theme="light">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" class="hljs-theme" data-theme="dark" disabled>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/marked/12.0.0/marked.min.js"></script>
<link rel="stylesheet" href="./style.css">
</head>
<body>

<header class="header">
  <button class="menu-toggle" onclick="toggleMobile()">&#9776;</button>
  <div class="header-brand">
    <div class="header-logo">Go</div>
    <span class="header-title">Go Concurrency</span>
    <div class="header-sep"></div>
    <span class="header-part-title" id="headerPart">Select a section to begin</span>
  </div>
  <div class="section-nav" id="sectionNav" style="display:none">
    <button class="nav-btn" id="prevSection" onclick="prevSlide()">&#8249;</button>
    <span class="section-indicator" id="sectionIndicator">1 / 1</span>
    <button class="nav-btn" id="nextSection" onclick="nextSlide()">&#8250;</button>
  </div>
  <div class="header-right">
    <button class="nav-btn font-btn" onclick="changeFontSize(-1)" title="Decrease font size">A&#8722;</button>
    <button class="nav-btn font-btn" onclick="changeFontSize(1)" title="Increase font size">A+</button>
    <button class="nav-btn" id="darkToggle" onclick="toggleDark()" title="Toggle dark mode"><span id="darkIcon">&#9789;</span></button>
    <button class="nav-btn" title="Toggle sidebar" onclick="toggleSidebar()" style="font-size:14px">&#9776;</button>
  </div>
</header>

<div class="layout">
  <nav class="sidebar" id="sidebar">
    <div id="sidebarContent">
      <div class="loading"><div class="spinner"></div><div class="loading-text">Loading...</div></div>
    </div>
  </nav>

  <div class="main" id="mainArea">
    <div class="sections-container" id="sectionsContainer">
      <div class="welcome" id="welcomeScreen">
        <div class="welcome-icon">Go</div>
        <h1>Go Concurrency Mastery</h1>
        <p>A hands-on course to master concurrent programming in Go. Navigate sections horizontally with arrow keys or buttons.</p>
        <button class="welcome-btn" onclick="loadFirst()">Start Reading</button>
        <a class="welcome-btn resume-btn" id="resumeBtn" style="display:none" href="#">Continue Reading</a>
        <div class="welcome-hint">Use &#8592; &#8594; arrow keys to navigate sections</div>
      </div>
    </div>
    <div class="section-dots" id="sectionDots"></div>
  </div>
</div>

<script>
var __BASE_PATH__ = ".";
var __PAGE_ID__ = "_overview/COMPLETE_COURSE_CURRICULUM.md";
var __PAGE_CONTENT__ = "# Go Concurrency Mastery: Complete Course Curriculum\n## 100% Problem-First, Beginner-to-Expert Progression\n\n**Last Updated**: February 2026\n\n**Total Duration**: 180-240 hours for complete mastery\n\n---\n\n## Course Philosophy\n\n**Learning Method**: PROBLEM → CODE → BREAK → FIX → UNDERSTAND → MASTER\n\nEvery chapter follows this proven structure:\n1. **Real Problem** — Something you'd encounter in production\n2. **Naive Solution** — Simple code that works (or breaks)\n3. **Watch It Break** — See failures in action with race detector, pprof, panics\n4. **Deep Understanding** — WHY it broke, with ASCII diagrams and analogies\n5. **Fix It Multiple Ways** — Different solutions with explicit trade-offs\n6. **Go Deep** — How it works internally (beginner-friendly level of detail)\n7. **Practice Problems** — 10–30 exercises per chapter, solutions with explanations\n8. **Production Considerations** — Real codebases (Docker, Kubernetes, etcd, NSQ)\n\n**No theory lectures. No pseudocode. Just runnable code that teaches.**\n\n---\n\n# PART 1: SINGLE-PROCESS CONCURRENCY\n\nMaster Go's concurrency primitives within one program before tackling distributed systems.\n\n---\n\n## Chapter 0: The Concurrency Foundations\n\n**Problem**: Your web server handles 1 request/sec sequentially. It needs to handle 10,000.\n**Duration**: 3–4 hours\n**Difficulty**: Beginner\n**Practice Problems**: 5\n**Purpose**: Build the mental model before touching a single primitive\n\n\u003e This chapter does NOT teach goroutines yet. It teaches you HOW TO THINK about concurrency — the vocabulary, the model, and the intuition — so everything in Chapters 1–19 clicks immediately instead of feeling like magic.\n\n### Section Breakdown\n\n#### Part 0: The Motivation Problem (30 min)\n- 0.1: The Single-Threaded Coffee Shop (analogy: one barista, one customer at a time)\n- 0.2: Adding More Baristas (concurrency) vs. Bigger Espresso Machine (parallelism)\n- 0.3: Why the Distinction Matters in Go\n- 0.4: Our First Benchmark — sequential URL fetcher (11 seconds for 10 URLs)\n- 0.5: The Goal — same result in 1.1 seconds (sneak preview, no code explained yet)\n\n#### Part 1: Concurrency Is Not Parallelism (45 min)\n- 1.1: Rob Pike's Gopher Book-Burning Analogy (relay: one gopher → two → pipeline)\n- 1.2: Sequential: one gopher, all the work\n- 1.3: Concurrent: multiple gophers, coordinated — but still one CPU track\n- 1.4: Parallel: multiple gophers, multiple CPU tracks simultaneously\n- 1.5: Code proof — GOMAXPROCS=1 vs GOMAXPROCS=NumCPU, Monte Carlo π simulation\n- 1.6: The Key Insight: concurrency is about structure, parallelism is about execution\n- 1.7: When concurrency helps even on one CPU (I/O-bound work)\n- 1.8: Quiz: CPU-bound vs I/O-bound identification (5 scenarios)\n\n#### Part 2: Communicating Sequential Processes — Go's Philosophy (30 min)\n- 2.1: The Shared-Memory Approach (most languages) — shared box everyone grabs from\n- 2.2: The CSP Approach (Go's way) — relay race, hand off data via meeting points\n- 2.3: Tony Hoare's insight (1978): processes + channels, not threads + locks\n- 2.4: \"Don't communicate by sharing memory; share memory by communicating\"\n- 2.5: Go's URL Poller: shared map + mutex version vs channel version (side by side)\n- 2.6: Why Go chose CSP (not a replacement for mutexes — a complementary model)\n\n#### Part 3: The GMP Scheduler — What Actually Runs Your Code (45 min)\n- 3.1: The problem with OS threads (1MB+ stack, slow context switch)\n- 3.2: Goroutines: ~2KB stack, scheduled by Go runtime, not OS\n- 3.3: G — the goroutine (your unit of work)\n- 3.4: M — the machine (OS thread, what the OS sees)\n- 3.5: P — the processor (scheduling context, owns a local run queue)\n- 3.6: How P picks up work: local queue → global queue → steal from another P\n- 3.7: What happens when a goroutine blocks on a channel (G parks, P continues)\n- 3.8: What happens when a goroutine blocks on a syscall (M detaches from P)\n- 3.9: Preemption since Go 1.14 (SIGURG, 10ms time slice — no more goroutine monopolies)\n- 3.10: GOMAXPROCS: defaults to CPU count, when to change it\n- 3.11: ASCII diagram: G/M/P relationships and work stealing\n- 3.12: The practical payoff — why this explains latency patterns you'll debug later\n\n#### Part 4: Failure Modes Overview — The Map of Problems (20 min)\n- 4.1: Race Condition — two goroutines fight over data (Ch 1)\n- 4.2: Deadlock — everyone is waiting for everyone else (Ch 2)\n- 4.3: Livelock — everyone is \"working\" but no progress (Ch 2)\n- 4.4: Starvation — one goroutine never gets a turn (Ch 2)\n- 4.5: Goroutine Leak — goroutine runs forever, consuming memory (Ch 4)\n- 4.6: Visual map: which primitive prevents which failure mode\n- 4.7: The decision tree preview (Mutex? Channel? Atomic? — answered in detail in Ch 1)\n\n#### Part 5: Practice Problems (45 min)\n1. GOMAXPROCS experiment: measure CPU-bound speedup at 1, 2, 4, 8\n2. I/O-bound experiment: measure concurrent vs sequential HTTP fetches\n3. Identify the failure mode in 5 code snippets\n4. Sequential-to-concurrent URL fetcher (no channels yet — just `go` and `time.Sleep`)\n5. Explain CSP vs shared memory in your own words, then compare the two URL poller implementations\n\n**Interview Questions Covered:**\n- \"What's the difference between concurrency and parallelism?\" (All companies)\n- \"How does Go schedule goroutines?\" (Google/Uber senior)\n- \"Why does Go use CSP instead of shared memory?\" (Staff+ design questions)\n\n---\n\n## Chapter 1: The Race Condition Crisis\n\n**Problem**: Build a web analytics URL hit counter\n**Duration**: 12–16 hours for mastery\n**Difficulty**: Beginner\n**Practice Problems**: 15\n\n### Section Breakdown\n\n#### Part 0: Introduction (10 min)\n- 0.1: What We're Building\n- 0.2: The Challenge\n- 0.3: Our Learning Journey\n- 0.4: What You'll Learn\n- 0.5: How to Use This Chapter\n- 0.6: Prerequisites Check (Chapter 0 required)\n- 0.7: A Note on Learning\n\n#### Part 1: The Sequential Baseline (30 min)\n- 1.1: The Problem Statement\n- 1.2: Sequential Implementation (complete, runnable code)\n- 1.3: Running and Timing the Code\n- 1.4: Measuring the Bottleneck\n- 1.5: Understanding Why It's Slow\n- 1.6: Stop and Think (quizzes)\n- 1.7: What's Next\n\n#### Part 2: Breaking It — Naive Concurrent Version (40 min)\n- 2.1: Let's Make It Concurrent!\n- 2.2: What Are Goroutines? (from scratch)\n- 2.3: Naive Concurrent Implementation\n- 2.4: Running the Broken Code\n- 2.5: The Race Detector (`go run -race`)\n- 2.6: Understanding the Race Detector Output\n- 2.7: Exercises\n\n#### Part 3: What IS a Race Condition? Deep Dive (60 min)\n- 3.1: The Read-Modify-Write Cycle\n- 3.2: The Lost Update Problem (with CPU instruction timeline)\n- 3.3: Memory, Caches, and Why CPUs Lie (beginner-friendly)\n- 3.4: The Three Conditions for a Race\n- 3.5: Types of Races (data race, logic race, ordering race)\n- 3.6: Practice Identifying Races (10 examples)\n\n#### Part 4: Solution #1 — Channels (130 min)\n- 4.1: What IS a Channel? (mailbox analogy)\n- 4.2: Your First Channel (send, receive, types)\n- 4.3: Channels Block! (unbuffered synchronization)\n- 4.4: Buffered Channels (the parking lot analogy)\n- 4.5: Buffered vs Unbuffered — when each is right\n- 4.6: Channel-Based Counter Solution\n- 4.7: How Does This Prevent the Race? (single owner goroutine)\n- 4.8: Closing Channels (`close(ch)`, reading from closed)\n- 4.9: Ranging Over Channels (`for v := range ch`)\n- 4.10: How Channels Work Internally (hchan, lock, sendq/recvq — simplified)\n- 4.11: Channel Performance (200–300 ns vs mutex's 15–20 ns — when that matters)\n- 4.12: The Select Statement\n- 4.13: sync.Once — One-Time Initialization\n- 4.14: When to Use Channels\n- 4.15: Common Channel + Select Patterns\n- 4.16: Common Mistakes\n- 4.17: Exercises (15 problems)\n\n#### Part 5: Solution #2 — Mutexes (120 min)\n- 5.1: What IS a Mutex? (bathroom key analogy)\n- 5.2: Your First Mutex\n- 5.3: The Critical Section\n- 5.4: Why `defer Unlock()`?\n- 5.5: Mutex-Based Counter Solution\n- 5.6: How Mutex Prevents the Race\n- 5.7: Locking for Reads Too\n- 5.8: How Mutexes Work Internally (state bits, sema, spin loop)\n- 5.9: Normal Mode vs Starvation Mode (Go 1.9's fix for goroutine starvation)\n- 5.10: Mutex Performance\n- 5.11: When to Use Mutexes\n- 5.12: RWMutex — Better for Read-Heavy Workloads\n- 5.13: Common Mistakes\n- 5.14: Exercises (15 problems)\n\n#### Part 6: Solution #3 — Atomics (80 min)\n- 6.1: What ARE Atomic Operations?\n- 6.2: Your First Atomic\n- 6.3: All Atomic Operations Available\n- 6.4: Atomic-Based Counter Solution\n- 6.5: How Atomics Prevent the Race (CPU LOCK prefix)\n- 6.6: CPU-Level Atomics (beginner-friendly x86 explanation)\n- 6.7: Atomic Performance (3–5 ns — fastest option)\n- 6.8: When to Use Atomics\n- 6.9: Common Mistakes (ABA problem, wrong type size)\n- 6.10: Exercises (10 problems)\n\n#### Part 7: Comparing All Three Solutions (50 min)\n- 7.1: Side-by-Side Code Comparison\n- 7.2: Benchmarking All Three (`go test -bench`)\n- 7.3: The Decision Tree: Channel? Mutex? Atomic?\n- 7.4: Production Considerations\n- 7.5: Real-World Examples (Kubernetes counter, etcd state machine, NSQ queue)\n\n#### Part 8: Practice Problems (120 min)\n- Beginner: 5 problems\n- Intermediate: 5 problems\n- Advanced: 5 problems\n\n#### Part 9: Wrap-Up (10 min)\n- 9.1: What You Learned (race conditions + all three solutions + select + sync.Once)\n- 9.2: Cheat Sheet\n- 9.3: Next Chapter Preview\n\n**Interview Questions Covered:**\n- \"Fix this race condition\" (All FAANG)\n- \"Implement thread-safe counter\" (Google/Meta)\n- \"When to use channels vs mutexes?\" (Amazon/Uber)\n- \"What does select do when multiple cases are ready?\" (Google/Netflix senior)\n- \"Explain sync.Once implementation\" (Meta/senior roles)\n\n---\n\n## Chapter 2: The Deadlock Disaster\n\n**Problem**: Multi-account money transfer system\n**Duration**: 16–20 hours\n**Difficulty**: Beginner-Intermediate\n**Practice Problems**: 22 (was 18)\n\n### Section Breakdown\n\n#### Part 1: Introduction \u0026 Sequential Transfers (30 min)\n- 1.1: The Problem — transfer between any two accounts, 1000 concurrent transfers\n- 1.2: Sequential Implementation (works, but slow)\n- 1.3: Naive Concurrent Attempt\n\n#### Part 2: Deadlock — The Program That Freezes (40 min)\n- 2.1: Running the Broken Code (program hangs forever)\n- 2.2: Reading the Deadlock Panic Message\n- 2.3: Understanding What Happened (goroutine dump)\n\n#### Part 3: What IS a Deadlock? Deep Dive (60 min)\n- 3.1: The Four Coffman Conditions (mutual exclusion, hold-and-wait, no preemption, circular wait)\n- 3.2: Deadlock Visualized (resource allocation graph, circular dependency diagram)\n- 3.3: Go's Built-In Deadlock Detection (runtime check — only catches ALL goroutines blocked)\n- 3.4: Partial Deadlocks — the harder-to-detect case\n- 3.5: Real Deadlock in Docker (container start failure, 6 leaked goroutines)\n- 3.6: Quiz: spot the deadlock in 5 code snippets\n\n#### Part 4: Solution #1 — Lock Ordering (60 min)\n- 4.1: The Insight — always lock in the same global order (by account ID)\n- 4.2: Implementation with ordered locking\n- 4.3: Why This Breaks Circular Wait (Coffman condition #4)\n- 4.4: Lock Ordering in the Wild (Linux kernel, database systems)\n- 4.5: Exercises: 3 problems\n\n#### Part 5: Solution #2 — Timeout with Select (60 min)\n- 5.1: TryLock in Go 1.18+ (`sync.Mutex.TryLock()`)\n- 5.2: Implementation: try → timeout → retry with backoff\n- 5.3: Trade-offs: complexity, correctness under high contention\n- 5.4: Exercises: 3 problems\n\n#### Part 6: Solution #3 — Try-Lock Pattern (60 min)\n- 6.1: The Context-Based Timeout Approach\n- 6.2: Implementation: context + select for lock acquisition\n- 6.3: Production Pattern: transferWithContext\n- 6.4: Exercises: 3 problems\n\n#### Part 7: Classic Problem — Dining Philosophers (90 min)\n- 7.1: Problem Statement (5 philosophers, 5 chopsticks, eat or think)\n- 7.2: Naive Solution → deadlock\n- 7.3: Solution A: Lock Ordering (one left-handed philosopher)\n- 7.4: Solution B: Resource Hierarchy (arbitrator/waiter pattern)\n- 7.5: Solution C: Chandy-Misra algorithm\n- 7.6: Exercises: implement all three solutions\n\n#### Part 8: Detecting Deadlocks in Production (40 min)\n- 8.1: `go tool pprof` goroutine dump\n- 8.2: Reading \"all goroutines are asleep\" output\n- 8.3: Writing tests that time out to catch deadlocks\n- 8.4: `goleak` for goroutine state analysis\n\n####  Part 9 — Livelock (60 min)\n- 9.1: Introducing the Problem — two goroutines politely yielding to each other forever\n- 9.2: The First Livelock — TryLock without backoff\n- 9.3: Running the Broken Code — CPU at 100%, no transfers complete\n- 9.4: What Makes Livelock Different from Deadlock\n- 9.5: The Detection Challenge — no runtime panic, pprof shows retry loops\n- 9.6: Real-World Livelock Pattern — database optimistic locking storm\n- 9.7: Fix #1 — Randomized Exponential Backoff with Jitter\n- 9.8: Fix #2 — Lock Ordering (eliminates the symmetric retry)\n- 9.9: Fix #3 — Context timeout (give up after N ms, return error to caller)\n- 9.10: Real Case: Zabbix Server livelock (history syncers, 100% CPU, server appears alive but processes nothing)\n- 9.11: Livelock vs Deadlock vs Starvation — comparison table\n- 9.12: Exercises: 3 livelock problems (reproduce → diagnose → fix)\n\n#### Part 10 — Starvation (60 min)\n- 10.1: Introducing the Problem — a \"greedy\" goroutine that monopolizes a mutex\n- 10.2: Running the Broken Code — one goroutine gets millions of locks, another gets ten\n- 10.3: Measuring Starvation — per-goroutine lock acquisition counter\n- 10.4: Go's Mutex Starvation Mode (Go 1.9 fix for issue #13086)\n- 10.5: Goroutine Starvation in the Scheduler\n- 10.6: Channel Scheduling Starvation — issue #6205, multiple writers on one channel\n- 10.7: How to Detect Starvation (per-goroutine metrics, work distribution histogram)\n- 10.8: How to Fix Starvation (fair queues, rate limiting per-goroutine, work stealing)\n- 10.9: Exercises: 3 starvation problems\n\n#### Part 11: The Full Failure-Mode Taxonomy (30 min)\n- 11.1: Side-by-side comparison — Deadlock / Livelock / Starvation / Race Condition\n- 11.2: Detection method for each\n- 11.3: Fix strategy for each\n- 11.4: Which Go primitives prevent which\n\n#### Part 12: Practice Problems (180 min)\n- Beginner: 5 problems\n- Intermediate: 5 problems\n- Advanced: 5 problems (include livelock and starvation scenarios)\n\n#### Part 13: Wrap-Up (20 min)\n- 13.1: What You Learned\n- 13.2: Cheat Sheet\n- 13.3: Next Chapter Preview\n\n**Interview Questions Covered:**\n- \"Solve Dining Philosophers\" (Amazon/Google)\n- \"Implement safe concurrent transfers\" (Goldman Sachs)\n- \"Detect deadlocks in code\" (Microsoft)\n- \"What's the difference between deadlock, livelock, and starvation?\" (All FAANG senior)\n- \"How would you detect a livelock in production?\" (Netflix/Uber staff)\n\n---\n\n## Chapter 3: The Channel Catastrophe\n\n**Problem**: Log processing pipeline (producer-consumer)\n**Duration**: 14–18 hours\n**Difficulty**: Intermediate\n**Practice Problems**: 20\n\n### What You'll Build\n- Log processing pipeline: ingest → parse → filter → aggregate\n- Multiple producers, multiple consumers\n- Channel patterns: fan-out, fan-in, pipelines with proper close handling\n\n### Major Sections\n1. **Introduction \u0026 Sequential Pipeline** (30 min)\n2. **Unbuffered Channels** — no concurrency benefit (40 min)\n3. **Buffered Channels** — goroutine leaks! (60 min)\n4. **Channel Closing** — panics and deadlocks (90 min)\n5. **Multiple Producers** — safe close with sync.Once (60 min)\n6. **Proper Pattern: WaitGroup + Once Closer** (90 min)\n7. **Fan-Out Pattern** (60 min)\n8. **Fan-In Pattern** (60 min)\n9. **Pipeline Pattern with Close Cascade** (90 min)\n10. **Real-World Deep Dive: Docker Goroutine Leaks** (60 min)\n    - Issue #13809: 201,482 accumulated goroutines from missing cancellation\n    - LeakProf: 10 leaks/req × 1000 req/sec = system collapse\n11. **Practice Problems** (180 min)\n12. **Wrap-Up** (30 min)\n\n**Key Concepts:**\n- Channel direction (send-only, receive-only)\n- close() propagation cascade\n- or-done-channel pattern (Cox-Buday)\n- Done channel as broadcast signal\n\n**Interview Questions:**\n- \"Build a worker pool\" (Uber/Netflix)\n- \"Implement pipeline with cancellation\" (Google)\n- \"Fix channel deadlocks\" (Meta/Airbnb)\n\n---\n\n## Chapter 4: The Goroutine Leak Nightmare\n\n**Problem**: HTTP API making concurrent external calls\n**Duration**: 12–16 hours\n**Difficulty**: Intermediate\n**Practice Problems**: 16\n\n### What You'll Build\n- API gateway calling multiple services\n- Memory grows unbounded (leak!)\n- Fix with context and proper cleanup\n\n### Major Sections\n1. **Introduction \u0026 Simple API** (30 min)\n2. **Spawning Goroutines** — looks fine (40 min)\n3. **Load Testing** — memory explodes! (60 min)\n4. **Finding Leaks with pprof** (90 min)\n5. **Leak Pattern #1: Blocking Channel** (60 min)\n6. **Leak Pattern #2: HTTP Without Timeout** (60 min)\n7. **Leak Pattern #3: Blocked Select** (60 min)\n8. **Leak Pattern #4: time.After in Loop** (30 min) ← the GC-before-expiry gotcha\n9. **Fix with context.Context** (120 min)\n10. **Goroutine Leak Detection Tools** (45 min)\n    - `runtime.NumGoroutine()` — the canary\n    - `uber-go/goleak` for tests\n    - pprof goroutine endpoint in production\n11. **Practice Problems** (150 min)\n12. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Goroutine lifecycle\n- Context propagation\n- Timeout patterns (time.After vs time.NewTimer)\n- Cancellation\n\n**Interview Questions:**\n- \"How do you prevent goroutine leaks?\" (All FAANG)\n- \"Implement request timeout\" (Netflix/Stripe)\n- \"Debug this memory leak\" (Uber real code)\n\n---\n\n## Chapter 5: The WaitGroup Wreckage\n\n**Problem**: Coordinate parallel database queries\n**Duration**: 10–12 hours\n**Difficulty**: Beginner-Intermediate\n**Practice Problems**: 14\n\n### What You'll Build\n- Parallel queries to multiple databases\n- Collect results, handle errors\n- errgroup package patterns\n\n### Major Sections\n1. **Introduction \u0026 Sequential Queries** (30 min)\n2. **Firing Goroutines** — missing results! (40 min)\n3. **WaitGroup Basics** (60 min)\n4. **The Five Common WaitGroup Bugs** (90 min)\n   - Add after Done\n   - Add inside the goroutine\n   - Reuse before Wait completes\n   - Passing WaitGroup by value\n   - Missing Done (defer is your friend)\n5. **Collecting Results with Channels** (60 min)\n6. **Collecting Results with Mutex** (60 min)\n7. **Error Handling Patterns** (90 min)\n   - First error wins (context cancellation)\n   - Collect all errors\n8. **The errgroup Package** (90 min)\n   - errgroup.Group basics\n   - errgroup.WithContext — the gold standard\n9. **Practice Problems** (120 min)\n10. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- WaitGroup Add/Done/Wait\n- Error aggregation patterns\n- errgroup.WithContext\n\n**Interview Questions:**\n- \"Coordinate N parallel operations\" (Google/Amazon)\n- \"Implement parallel error handling\" (Meta)\n- \"Fix WaitGroup bugs\" (Microsoft/Apple)\n\n---\n\n## Chapter 6: The Select Statement Sorcery\n\n**Problem**: Job scheduler with priorities and timeouts\n**Duration**: 12–14 hours\n**Difficulty**: Intermediate\n**Practice Problems**: 17\n\n### What You'll Build\n- Priority job scheduler (high-priority jobs must preempt low-priority ones)\n- Timeout handling with graceful degradation\n- Full cancellation pipeline using select\n\n### Major Sections\n1. **Introduction \u0026 Basic Scheduler** (30 min)\n2. **Multiple Channels** — select basics recap + deepen (60 min)\n3. **Random Selection Problem** — \"I expected FIFO but got random!\" (60 min)\n   - Why Go chose pseudo-random (fairness, deadlock avoidance)\n   - How to measure actual distribution (10,000-iteration fairness test)\n4. **Priority Select Patterns** (90 min)\n   - Double-select trick (inner/outer nested select)\n   - When double-select still isn't guaranteed\n   - Using a single command channel with priority tags as the robust alternative\n   - Real-world: Kubernetes job queue priority\n5. **Timeout with time.After** — the hidden memory leak (90 min)\n   - Each call creates a new timer channel — not GC'd until fired\n   - Real case: ArangoDB ~1GB leak, 60K msg/sec → 18 million pending timers\n   - Fix: time.NewTimer + Reset() outside the loop\n   - Go 1.23: time.After now GC-eligible (explain the change)\n6. **Proper Timeout with time.Timer** (60 min)\n7. **Context Cancellation Deep Dive** (90 min)\n   - context.Done() is just a channel — it fits select naturally\n   - Pattern: every blocking loop checks `\u003c-ctx.Done()`\n   - Building a cancellable worker pool\n8. **Or-Channel Pattern** (60 min) — combining multiple done signals\n   - Recursive or() function from Cox-Buday\n   - Real use: \"cancel if ANY of these conditions fires\"\n9. **Select with Nil Channels — Dynamic Case Toggling** (45 min)\n   - Complete merger example using nil-on-close pattern\n10. **select `default` Performance Trap** (30 min)\n    - Busy loop with empty default: 100% CPU\n    - When default is actually right (polling, non-blocking check)\n11. **Practice Problems** (150 min)\n12. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Select pseudorandom specification\n- time.After GC behavior (pre/post 1.23)\n- Timer.Reset pattern\n- Context cancellation\n\n**Interview Questions:**\n- \"Implement timeout with cancellation\" (Uber/Stripe)\n- \"Build a rate limiter\" (All FAANG + Stripe)\n- \"What happens when multiple select cases are ready?\" (Google/Netflix — very common)\n- \"Fix this time.After memory leak\" (senior roles)\n\n---\n\n## Chapter 7: The Mutex Mayhem\n\n**Problem**: Thread-safe LRU cache for API responses\n**Duration**: 14–18 hours\n**Difficulty**: Intermediate-Advanced\n**Practice Problems**: 19\n\n### What You'll Build\n- LRU cache from scratch\n- Compare single lock, RWMutex, sharded locks\n- Understand lock granularity through benchmarking\n\n### Major Sections\n1. **Introduction \u0026 Naive Map** — concurrent map panic (40 min)\n2. **Single Mutex Solution** (60 min)\n3. **Performance Problem** — reads block writes block reads! (60 min)\n4. **RWMutex Solution** — multiple readers, exclusive writer (90 min)\n5. **Lock Granularity** — protecting too much vs too little (90 min)\n6. **Sharded Map Pattern** — split into N buckets, each with its own lock (120 min)\n7. **sync.Map Comparison** (60 min)\n   - Internal two-map structure (read map + dirty map)\n   - Benchmark crossover: sync.Map wins at 95%+ reads, loses at 50/50\n   - Decision rule: use sync.Map for write-once/read-many\n8. **Deadlock with Multiple Locks** — revisit Ch 2 with real LRU (90 min)\n9. **TryLock (Go 1.18+)** — when and how to use it safely\n10. **Practice Problems** (180 min)\n11. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Critical section minimization\n- Read-write lock semantics\n- Lock striping/sharding\n- sync.Map use cases\n\n**Interview Questions:**\n- \"Implement thread-safe LRU cache\" (Google/Meta/Amazon — classic)\n- \"When RWMutex vs Mutex?\" (All senior roles)\n- \"Design high-concurrency cache\" (Netflix/Uber)\n- \"When would you use sync.Map?\" (senior roles)\n\n---\n\n## ⭐ NEW — Chapter 8: The Sync Toolbox Crisis\n\n**Problem**: A high-throughput HTTP server with 3 separate bugs, each requiring a different sync tool\n**Duration**: 14–16 hours\n**Difficulty**: Intermediate\n**Practice Problems**: 18\n\n### What You'll Build\nBy the end, you'll have a production-grade HTTP server that uses all four tools correctly:\n- `sync.Once`: safe lazy initialization of a database connection pool\n- `sync.Pool`: eliminating GC pressure from 10,000 JSON encoder allocations/second\n- `sync.Cond`: a bounded work queue where producers wait for space\n- `sync.Map`: a high-read-ratio registry with disjoint key access\n\n### Section Breakdown\n\n#### Part 1: Introduction (20 min)\n- 1.1: The Four Problems the server has (each exposed by load testing)\n- 1.2: Why channels and regular mutexes can't solve all of them\n- 1.3: The sync package philosophy — specialized tools for specialized problems\n\n#### Part 2: sync.Once — The Singleton Race (90 min)\n\n**Problem**: Your `GetDB()` function is called by 10,000 goroutines at startup. Without protection, the database connects 10,000 times.\n\n- 2.1: The broken code — `if db == nil { db = connect() }` under race detector\n- 2.2: First fix attempt — `mu.Lock()` wrapping (works but slow every call forever)\n- 2.3: Double-check locking — broken without atomics (the Go memory model trap)\n- 2.4: sync.Once — the correct, fast solution\n  ```go\n  var once sync.Once\n  var db *DB\n  func GetDB() *DB {\n      once.Do(func() { db = connect() })\n      return db\n  }\n  ```\n- 2.5: How sync.Once works internally\n  - Fast path: `atomic.LoadUint32(\u0026o.done)` — near-zero overhead after first call\n  - Slow path: mutex + double-check + `defer atomic.StoreUint32(\u0026o.done, 1)`\n  - Why CAS alone is wrong: second caller returns before first caller's f() completes\n  - The `done` field is first in struct for cache-line alignment on 64-bit systems\n- 2.6: Go 1.21 additions: sync.OnceValue, sync.OnceFunc, sync.OnceValues\n  - `sync.OnceValue[*DB](connect)` — caches the return value, zero boilerplate\n  - `sync.OnceValues[*DB, error](connectWithErr)` — caches (result, error) pair\n- 2.7: Gotchas\n  - Recursive Do: `once.Do(func() { once.Do(other) })` → deadlock\n  - No retry on error: init is marked done even if f() fails (use OnceValue workaround)\n  - Don't copy Once after first use (vet tool catches this)\n  - Calling different functions on same Once (second one never runs)\n- 2.8: Production usage: Kubernetes metrics registry (sync.Once prevents double-registration panic)\n- 2.9: init() vs package-level var vs sync.Once — decision framework\n- 2.10: Exercises: 4 sync.Once problems\n\n#### Part 3: sync.Pool — GC Pressure Elimination (90 min)\n\n**Problem**: Under load, your server allocates 10,000 `bytes.Buffer` objects per second for JSON encoding. GC pauses are visible in latency percentiles.\n\n- 3.1: The broken code — `buf := new(bytes.Buffer)` inside the hot path\n- 3.2: pprof allocation profile — 80% of allocations are these buffers\n- 3.3: The fix — sync.Pool\n  ```go\n  var bufPool = sync.Pool{\n      New: func() any { return new(bytes.Buffer) },\n  }\n  buf := bufPool.Get().(*bytes.Buffer)\n  buf.Reset() // ← critical: always reset before use\n  defer bufPool.Put(buf)\n  ```\n- 3.4: Benchmark: before/after — ~40x speedup, 1/5700th the memory allocation (real numbers from Go stdlib)\n- 3.5: How sync.Pool works internally\n  - Per-P (processor) local pool — no contention when P count ≤ GOMAXPROCS\n  - Shared pool for cross-P access (mutex-protected)\n  - Victim cache (Go 1.13+): objects survive at least two GC cycles\n  - Items ARE cleared at GC — pool is for temporary objects, not long-term storage\n- 3.6: The pool performance crossover\n  - Trivially cheap objects (\u003c10ns to allocate): pool overhead (~4ns) isn't worth it\n  - Expensive objects (buffers, codecs, TCP connections): pool wins by 10–100x\n- 3.7: Standard library references — fmt pools pp objects, net/http pools bufio readers, encoding/json pools encodeState\n- 3.8: Real Gotcha — Issue #23199: Memory livelock from large pooled buffers\n  - 2.5GB could not be freed because large burst-traffic buffers kept getting returned to pool\n  - Rule: pool items must have bounded, approximately equal memory cost\n  - Don't pool: dynamically-grown buffers that might reach 100MB\n- 3.9: The Reset discipline — what to reset before Put vs Get\n- 3.10: Exercises: 4 sync.Pool problems (measure GC, fix memory livelock)\n\n#### Part 4: sync.Cond — Condition Variables (90 min)\n\n**Problem**: Your bounded work queue: producers must wait when the queue is full, consumers must wait when the queue is empty. You want to wake ALL blocked producers when space frees up.\n\n- 4.1: First attempt with channels — `jobs := make(chan Job, 100)` with a fixed capacity\n- 4.2: The problem with channels for this: you can't dynamically resize, and broadcast on close is one-time only\n- 4.3: sync.Cond to the rescue\n  ```go\n  type BoundedQueue struct {\n      mu    sync.Mutex\n      cond  *sync.Cond\n      items []Job\n      max   int\n  }\n  // Producer\n  q.cond.L.Lock()\n  for len(q.items) == q.max { q.cond.Wait() }  // ← releases lock, sleeps, re-acquires on wake\n  q.items = append(q.items, job)\n  q.cond.Broadcast()\n  q.cond.L.Unlock()\n  ```\n- 4.4: The three operations — Wait (releases lock + sleep), Signal (wake one), Broadcast (wake all)\n- 4.5: Why Wait must be in a for-loop, not an if (spurious wakeups)\n- 4.6: When sync.Cond beats channels\n  - Broadcast is repeatable (closing a channel is one-time)\n  - Multiple goroutines waiting on different conditions under same mutex\n  - Zero allocation per signal (channel sends may allocate)\n- 4.7: Go team's honest assessment — issue #21165 proposed removing Cond in Go 2\n  - \"For many simple use cases, channels are better\"\n  - Only 42 packages in the Go corpus use sync.Cond\n  - Still irreplaceable for: in-memory pipe implementations, complex bounded buffers\n- 4.8: Production usage: net/http's pipe in h2_bundle.go, golang.org/x/net/http2\n- 4.9: Common mistakes: calling Wait outside lock, forgetting Broadcast after state change, using Signal when Broadcast needed\n- 4.10: sync.Cond vs channel vs semaphore — decision table\n- 4.11: Exercises: 4 sync.Cond problems (bounded buffer, readiness notification, multi-condition wait)\n\n#### Part 5: sync.Map — Concurrent Map for Specific Patterns (75 min)\n\n**Problem**: Your service registry is read by every request (10K/sec) and written when services register (1/sec). A regular `map + sync.RWMutex` shows as a hotspot in pprof.\n\n- 5.1: The broken code — concurrent map write panic (Go detects this and panics)\n- 5.2: First fix — `map + sync.Mutex` — correct but contended\n- 5.3: Better fix — `map + sync.RWMutex` — good for read-heavy workloads\n- 5.4: sync.Map — specialized for write-once/read-many patterns\n- 5.5: How sync.Map works internally\n  - `read` map: lock-free atomic snapshot, serves most reads\n  - `dirty` map: mutex-protected, receives all writes\n  - Promotion: when `misses` exceeds dirty map length, dirty promotes to read\n  - This means: bursty writes cause temporary contention until promotion\n- 5.6: Benchmark showdown — sync.Map vs RWMutex under varying read/write ratios\n  - 95% read / 5% write: sync.Map wins significantly\n  - 50% / 50%: RWMutex wins\n  - 10% read / 90% write: regular map + Mutex wins\n  - **Rule: use sync.Map when read ratio \u003e 80% OR disjoint key access per goroutine**\n- 5.7: sync.Map API quirks — no `len()`, no type safety, Range is not snapshot-consistent\n- 5.8: When NOT to use sync.Map (most cases)\n- 5.9: Real usage: Kubernetes informer cache, prometheus registry\n- 5.10: Exercises: 4 sync.Map problems (including benchmark-to-find-crossover)\n\n#### Part 6: Putting It All Together — The Fixed Server (30 min)\n- 6.1: The server before: 4 bugs, poor performance under load\n- 6.2: The server after: sync.Once for DB init, sync.Pool for encoders, sync.Cond for bounded queue, sync.Map for registry\n- 6.3: Load test comparison — before vs after (throughput, p99 latency, memory)\n\n#### Part 7: The Sync Package Decision Framework (20 min)\n- 7.1: Cheat sheet — which tool for which problem\n- 7.2: The hierarchy: channel \u003e mutex/rwmutex \u003e atomic \u003e once/pool/cond/map\n- 7.3: When ALL of these are the wrong tool (think about architecture)\n\n#### Part 8: Practice Problems (150 min)\n- Beginner: 5 problems\n- Intermediate: 7 problems\n- Advanced: 6 problems\n\n#### Part 9: Wrap-Up (20 min)\n\n**Interview Questions Covered:**\n- \"Explain sync.Once internals\" (Meta/Google senior)\n- \"When would you use sync.Pool?\" (All senior roles)\n- \"What's sync.Cond and when is it better than a channel?\" (Staff+ roles)\n- \"When to use sync.Map vs map + RWMutex?\" (Senior roles — benchmark answer expected)\n- \"How does Go's sync package relate to Java's java.util.concurrent?\" (cross-language roles)\n\n---\n\n## Chapter 9: The Context Confusion\n\n**Problem**: Microservice calling multiple downstream services\n**Duration**: 12–14 hours\n**Difficulty**: Intermediate\n**Practice Problems**: 15\n\n### What You'll Build\n- API aggregator calling 5 services\n- Proper timeout propagation\n- Graceful shutdown\n\n### Major Sections\n1. **Introduction \u0026 Naive Calls** (30 min)\n2. **Cascading Delays** — no cancellation (60 min)\n3. **Context Basics** — the tree model (90 min)\n4. **WithTimeout, WithDeadline, WithCancel** (120 min)\n5. **Context Propagation** — first argument convention (90 min)\n6. **Request-Scoped Values** — when to use (and when NOT to) (60 min)\n7. **Graceful Shutdown Pattern** (120 min)\n   - signal.NotifyContext (Go 1.16+)\n   - Drain pattern — finish in-flight, reject new\n8. **Context and select** — how `\u003c-ctx.Done()` fits the select patterns from Ch 6\n9. **Practice Problems** (150 min)\n10. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Context tree (parent cancels children)\n- Cancellation propagation\n- Value context (use sparingly — for request-scoped metadata only)\n- Shutdown patterns\n\n**Interview Questions:**\n- \"Implement graceful shutdown\" (All FAANG)\n- \"Handle cancellation across services\" (Uber/Netflix)\n- \"Build timeout hierarchy\" (Google/Stripe)\n\n---\n\n## Chapter 10: The Production Patterns Panic\n\n**Problem**: Production-ready web crawler\n**Duration**: 16–20 hours\n**Difficulty**: Advanced\n**Practice Problems**: 22\n\n### What You'll Build\n- Complete web crawler with: worker pools, rate limiting, deduplication, circuit breaker, metrics\n\n### Major Sections\n1. **Introduction \u0026 Simple Crawler** (40 min)\n2. **Worker Pool Pattern** (120 min)\n3. **Rate Limiter: Token Bucket** (90 min)\n   - `time.Ticker` + buffered channel semaphore\n   - `golang.org/x/time/rate` (leaky bucket, production standard)\n4. **Rate Limiter: Leaky Bucket** (90 min)\n5. **Deduplication** — singleflight pattern (90 min)\n   - `golang.org/x/sync/singleflight` — deduplicate in-flight requests\n   - Real usage: Kubernetes informer cache stampede prevention\n6. **Circuit Breaker** (120 min)\n   - States: Closed → Open → Half-Open\n   - Real case: Netflix Hystrix (the reference implementation)\n7. **Confinement Pattern** (60 min) *(from Cox-Buday — unique to this course)*\n   - Lexical confinement: pass a copy, not a reference\n   - Ad-hoc confinement: convention-based ownership\n   - Why confinement is the safest concurrency technique\n8. **Heartbeat Pattern** (60 min) *(from Cox-Buday)*\n   - Goroutine health monitoring via a heartbeat channel\n   - Real use: detecting hung workers in etcd\n9. **Metrics Collection** (90 min)\n10. **Practice Problems** (200 min)\n11. **Wrap-Up** (30 min)\n\n**Key Concepts:**\n- Bounded parallelism\n- Backpressure\n- Circuit breaker states\n- Request deduplication\n- Confinement and heartbeats\n\n**Interview Questions:**\n- \"Design a web crawler\" (Google classic)\n- \"Implement rate limiter\" (Stripe/Uber/All FAANG)\n- \"Build circuit breaker\" (Netflix/Amazon)\n\n---\n\n## Chapter 11: The Memory Model Mystery\n\n**Problem**: Understanding when writes become visible across goroutines\n**Duration**: 10–12 hours\n**Difficulty**: Advanced\n**Practice Problems**: 12\n\n### Major Sections\n1. **Introduction \u0026 Invisible Writes** (40 min)\n2. **The Go Memory Model** — the spec, in plain English (90 min)\n3. **Happens-Before Relationships** (120 min)\n4. **Channel Guarantees** — what the spec promises (60 min)\n5. **Mutex Guarantees** (60 min)\n6. **Atomic Guarantees** (60 min)\n7. **Common Memory Model Bugs** (90 min)\n8. **Double-Checked Locking** — broken vs fixed (60 min)\n9. **Practice Problems** (120 min)\n10. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Memory reordering\n- Cache coherence\n- Synchronization points\n- Sequential consistency\n\n**Interview Questions:**\n- \"Explain Go memory model\" (Senior+ all companies)\n- \"Why is this code wrong under the memory model?\" (Google/Netflix)\n- \"Implement lock-free counter correctly\" (Systems roles)\n\n---\n\n## Chapter 12: The Performance Profiling Path\n\n**Problem**: Concurrent code slower than sequential\n**Duration**: 10–12 hours\n**Difficulty**: Advanced\n**Practice Problems**: 13\n\n### Major Sections\n1. **Introduction \u0026 Slow Concurrent Code** (30 min)\n2. **Benchmarking Correctly** — avoiding pitfalls (90 min)\n3. **CPU Profiling with pprof** (90 min)\n4. **Mutex Contention Profiling** (90 min)\n5. **False Sharing** — cache lines and padding (90 min)\n6. **Amdahl's Law** — theoretical speedup limits (60 min)\n7. **CPU vs I/O Bound** — profiling to know which you have (60 min)\n8. **GOMAXPROCS Tuning** (60 min)\n9. **Mechanical Sympathy** *(from Ardan Labs)* — hardware awareness for better code (60 min)\n10. **Practice Problems** (120 min)\n11. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- pprof tools\n- Contention analysis\n- Cache line effects (64-byte alignment)\n- Speedup limits\n\n**Interview Questions:**\n- \"Why is concurrent code slow?\" (All senior roles)\n- \"Profile and optimize\" (Google/Meta/Netflix)\n- \"Explain Amdahl's Law\" (Systems design)\n- \"What is false sharing?\" (Low-level systems roles)\n\n---\n\n## Chapter 13: The Testing \u0026 Debugging Dungeon\n\n**Problem**: Production deadlocks and races — and tests that can't catch them\n**Duration**: 10–12 hours\n**Difficulty**: Advanced\n**Practice Problems**: 12\n\n### Major Sections\n1. **Introduction** (20 min)\n2. **Race Detector Deep Dive** — how it works, limitations, false negatives (90 min)\n3. **Testing Concurrent Code** *(NEW — from Zhiyanov)* (90 min)\n   - The challenge: concurrent bugs are non-deterministic\n   - `testing.T.Parallel()` for parallel test execution\n   - Stress testing with `-count=1000`\n   - `goleak` integration: `defer goleak.VerifyNone(t)` in every test\n   - Table-driven tests for concurrent functions\n   - Using `sync.WaitGroup` inside tests correctly\n4. **pprof Goroutine Profiling** (90 min)\n5. **Execution Tracing** (`go tool trace`) (90 min)\n6. **Delve for Goroutines** (60 min)\n7. **The Debugging Framework** — a systematic process for concurrency bugs (60 min)\n8. **Practice Problems** (120 min)\n9. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Race detector internals\n- Testing strategies for concurrent code\n- Goroutine analysis\n- Execution traces\n\n**Interview Questions:**\n- \"Debug concurrency issues\" (All companies)\n- \"Walk through debugging a deadlock\" (Google/Amazon)\n- \"How do you test concurrent code?\" (Senior roles — often asked, rarely answered well)\n\n---\n\n# PART 2: DISTRIBUTED CONCURRENCY\n\nApply concurrency concepts across multiple services and machines.\n\n---\n\n## Chapter 14: The Cross-Service Catastrophe\n\n**Problem**: Product price aggregator calling 10 supplier APIs\n**Duration**: 12–14 hours\n**Difficulty**: Intermediate-Advanced\n**Practice Problems**: 18\n\n### Major Sections\n1. **Introduction \u0026 Sequential Calls** (30 min)\n2. **Naive Parallel Calls** — hangs forever! (40 min)\n3. **Goroutine Leaks Across Services** — the distributed version (60 min)\n4. **errgroup.WithContext** — the standard solution (90 min)\n5. **Scatter-Gather Pattern** (90 min)\n6. **Circuit Breaker for Services** (120 min)\n7. **Request Hedging** — send redundant requests, take first response (90 min)\n8. **Partial Failure Handling** — some results are better than none (60 min)\n9. **Practice Problems** (180 min)\n10. **Wrap-Up** (20 min)\n\n**Key Concepts:**\n- Service orchestration\n- Partial failures\n- Redundant requests\n- Fail-fast patterns\n\n**Interview Questions:**\n- \"Aggregate data from multiple services\" (All FAANG)\n- \"Handle partial failures\" (Netflix/Uber)\n- \"Implement circuit breaker for service calls\" (Amazon/Google)\n\n---\n\n## Chapter 15: The Distributed Lock Disaster\n\n**Problem**: Scheduled job runs on 3 servers simultaneously\n**Duration**: 14–16 hours\n**Difficulty**: Advanced\n**Practice Problems**: 16\n\n### Major Sections\n1. **Introduction \u0026 Multi-Instance Problem** (40 min)\n2. **File Locking** — doesn't work across machines (60 min)\n3. **Redis SETNX** — lock never expires (60 min)\n4. **Redis with TTL** — still broken (split-brain case) (60 min)\n5. **Redis with Lua Script** — atomic check-and-set (90 min)\n6. **etcd Lease-Based Lock** (120 min)\n7. **Leader Election with etcd** (120 min)\n8. **Handling Split-Brain** — fencing tokens (90 min)\n9. **Practice Problems** (180 min)\n10. **Wrap-Up** (30 min)\n\n**Key Concepts:**\n- Distributed coordination\n- Lease semantics\n- Leader election\n- Split-brain scenarios\n\n**Interview Questions:**\n- \"Ensure one instance runs job\" (All FAANG)\n- \"Implement distributed locking\" (Google/Amazon)\n- \"Design leader election\" (Netflix/Uber)\n\n---\n\n## Chapter 16: The Saga Pattern Saga\n\n**Problem**: E-commerce order flow (Order → Payment → Inventory)\n**Duration**: 16–20 hours\n**Difficulty**: Advanced\n**Practice Problems**: 20\n\n### Major Sections\n1. **Introduction \u0026 Monolithic Transaction** (40 min)\n2. **Microservices Approach** — inconsistent state! (60 min)\n3. **Manual Rollback** — fails partway (60 min)\n4. **Saga Pattern Introduction** (90 min)\n5. **Compensating Transactions** (120 min)\n6. **Orchestration-Based Saga** (120 min)\n7. **Choreography-Based Saga** (120 min)\n8. **Idempotency** — handling retries safely (90 min)\n9. **Outbox Pattern** — guaranteed event delivery (90 min)\n10. **Practice Problems** (200 min)\n11. **Wrap-Up** (30 min)\n\n**Interview Questions:**\n- \"Handle distributed transactions\" (All FAANG)\n- \"Explain saga vs 2PC\" (Staff+ everywhere)\n- \"Design order processing\" (Amazon/Uber)\n\n---\n\n## Chapter 17: The Message Queue Madness\n\n**Problem**: Build your own message queue (NSQ/Kafka-like)\n**Duration**: 20–24 hours\n**Difficulty**: Advanced\n**Practice Problems**: 25\n\n### Major Sections\n1. **Introduction \u0026 Simple Queue** (40 min)\n2. **Race on Slice** — corruption! (60 min)\n3. **Mutex Protection** (60 min)\n4. **Multiple Topics** (90 min)\n5. **Buffered Channel as Queue** — NSQ's exact pattern (90 min)\n   - `memoryMsgChan chan *Message` IS the queue\n   - `--mem-queue-size` maps to channel buffer capacity\n6. **Bounded Buffer \u0026 Backpressure** (90 min)\n7. **Consumer Groups** (120 min)\n8. **Partitioning** (120 min)\n9. **Write-Ahead Log** (150 min)\n10. **Pub-Sub with Fan-Out** (90 min)\n11. **Practice Problems** (240 min)\n12. **Wrap-Up** (30 min)\n\n**Interview Questions:**\n- \"Design a message queue\" (Google/Amazon classic)\n- \"How does Kafka handle concurrency?\" (Uber/Netflix)\n- \"Implement pub-sub\" (All FAANG)\n\n---\n\n## Chapter 18: The Tracing Tragedy\n\n**Problem**: Request spans 5 services, can't find bottleneck\n**Duration**: 10–12 hours\n**Difficulty**: Intermediate-Advanced\n**Practice Problems**: 15\n\n### Major Sections\n1. **Introduction \u0026 Timeout Mystery** (30 min)\n2. **No Tracing** — impossible to debug (40 min)\n3. **Request ID Basics** (60 min)\n4. **Context Propagation** — trace IDs through goroutines (90 min)\n5. **Lost in Goroutines** — trace ID disappears in `go func()` (60 min)\n6. **HTTP Header Propagation** — W3C Trace Context standard (90 min)\n7. **Message Queue Metadata** — trace through async boundaries (90 min)\n8. **OpenTelemetry Integration** (120 min)\n9. **Distributed Spans** (90 min)\n10. **Practice Problems** (150 min)\n11. **Wrap-Up** (20 min)\n\n**Interview Questions:**\n- \"Debug distributed systems\" (All FAANG)\n- \"Implement distributed tracing\" (Google/Netflix)\n- \"Handle context propagation\" (Uber/Stripe)\n\n---\n\n## Chapter 19: The Healing System\n\n**Problem**: 50 worker goroutines. Some hang. Some crash. The system must self-heal.\n**Duration**: 8–10 hours\n**Difficulty**: Advanced\n**Practice Problems**: 10\n\n\u003e This chapter covers the most advanced production patterns that no other Go course teaches. It's the difference between a system that needs a restart at 3am vs one that heals itself.\n\n### What You'll Build\n- A worker pool that monitors its own health\n- Automatically replaces stuck or crashed workers\n- Error channels that propagate meaningful context to callers\n\n### Major Sections\n\n#### Part 1: The Problem — Silent Failures (30 min)\n- 1.1: Worker goroutine panics → entire program crashes (bad)\n- 1.2: Worker goroutine panics → silently ignored (also bad, just quieter)\n- 1.3: Worker goroutine hangs → jobs back up → memory grows → OOM\n- 1.4: The goal: detect, report, and recover from all three\n\n#### Part 2: Error Propagation at Scale (90 min)\n- 2.1: `chan error` for single-error reporting (from Chapters 3–5)\n- 2.2: Why this breaks at scale — errors lost, type information stripped\n- 2.3: The Error type with full context: timestamp, goroutine ID, stack trace\n- 2.4: Error pipelines — errors flow downstream like data\n- 2.5: Separating errors from values (two channels: `chan Result`, `chan error`)\n- 2.6: Implementation: log-and-continue vs propagate vs escalate\n\n#### Part 3: Heartbeat Pattern — Goroutine Health Monitoring (90 min)\n- 3.1: The problem — is that goroutine running, stuck, or dead?\n- 3.2: A heartbeat channel: goroutine sends a tick every N milliseconds\n- 3.3: Supervisor reads heartbeat; no heartbeat for 3× interval = goroutine is stuck\n- 3.4: Implementation: `heartbeat \u003c-chan struct{}` returned alongside results\n  ```go\n  func worker(done \u003c-chan struct{}, pulseInterval time.Duration) (\n      \u003c-chan struct{},  // heartbeat\n      \u003c-chan Result,   // results\n  )\n  ```\n- 3.5: Using heartbeats in tests — deterministic \"goroutine has started\" signal\n- 3.6: Real-world: etcd uses heartbeat channels between raft leader and followers\n\n#### Part 4: Healing Unhealthy Goroutines (90 min)\n- 4.1: The steward pattern — a goroutine that manages another goroutine\n- 4.2: Ward goroutine: does the actual work\n- 4.3: Steward goroutine: monitors ward, restarts on failure\n  ```go\n  func newSteward(timeout time.Duration, startWard wardFn) startFn\n  ```\n- 4.4: Panic recovery — defer recover inside ward, report via error channel\n- 4.5: Restart policies: always, on-failure, exponential-backoff-on-failure\n- 4.6: Supervision trees (Erlang's model, applied to Go)\n\n#### Part 5: Putting It Together — The Self-Healing Worker Pool (60 min)\n\n#### Part 6: Practice Problems (90 min)\n- 10 problems ranging from heartbeat implementation to full supervision tree\n\n#### Part 7: Wrap-Up (20 min)\n\n**Interview Questions Covered:**\n- \"How do you build a resilient worker pool?\" (Staff+ Netflix/Uber)\n- \"How would you handle goroutine panics in production?\" (Senior roles)\n- \"Design a self-healing service\" (Staff+ system design)\n\n---\n\n## Chapter 20: The Capstone Project\n\n**Problem**: Complete distributed e-commerce system\n**Duration**: 30–40 hours\n**Difficulty**: Expert\n**Practice Problems**: 30\n\n### System Architecture\n- Order Service (HTTP)\n- Payment Service (gRPC)\n- Inventory Service (REST)\n- Notification Service (async)\n- Your Message Queue (from Ch 17)\n\n### Features\nEvery concurrency pattern from all 19 chapters integrated:\n- Saga-based order flow with compensating transactions\n- Distributed tracing (OpenTelemetry)\n- Leader election for scheduled jobs\n- Circuit breakers for all service calls\n- Rate limiting per service\n- Distributed locking for inventory\n- Worker pools with heartbeat monitoring\n- Self-healing workers with supervision\n- Graceful shutdown with 30-second drain\n- sync.Pool for request object reuse\n- sync.Once for connection initialization\n- Metrics and health checks\n\n### Major Sections\n1. **Architecture \u0026 Service Skeleton** (180 min)\n2. **Message Queue Integration** (120 min)\n3. **Saga Implementation** (180 min)\n4. **Distributed Tracing** (120 min)\n5. **Leader Election** (120 min)\n6. **Circuit Breakers** (120 min)\n7. **Rate Limiting** (120 min)\n8. **Distributed Locks** (120 min)\n9. **Graceful Shutdown** (120 min)\n10. **Self-Healing Workers** (120 min)\n11. **Testing Strategy** (180 min)\n12. **Load Testing** (120 min)\n13. **Practice Problems** (360 min)\n14. **Wrap-Up** (60 min)\n\n---\n\n# COURSE STATISTICS\n\n## Time Investment\n\n| Category | Hours |\n|----------|-------|\n| **NEW** Chapter 0: Foundations | 3–4 hours |\n| Chapter 1: Race Conditions (enhanced) | 12–16 hours |\n| Chapter 2: Deadlock + Livelock + Starvation (enhanced) | 16–20 hours |\n| Chapters 3–7: Channel, Leak, WaitGroup, Select, Mutex | 62–78 hours |\n| **NEW** Chapter 8: Sync Toolbox | 14–16 hours |\n| Chapters 9–13: Context, Patterns, Memory, Perf, Debug | 48–58 hours |\n| Chapters 14–19: Distributed + Healing | 68–84 hours |\n| Chapter 20: Capstone | 30–40 hours |\n| **Total Course** | **253–316 hours** |\n\n**Realistic Schedule:**\n- **10 hours/week**: 6–8 months\n- **20 hours/week**: 3–4 months\n- **40 hours/week (bootcamp)**: 7–9 weeks\n\n## Content Volume\n\n| Metric | Count |\n|--------|-------|\n| Total Chapters | 21 (was 19) |\n| Total Sections | 600+ (was 500+) |\n| Practice Problems | 318+ (was 280+) |\n| Interview Questions | 100+ (was 85+) |\n| Code Examples | 350+ (was 300+) |\n| Lines of Code (you'll write) | 30,000+ (was 25,000+) |\n| Production Patterns | 50+ (was 40+) |\n\n## Difficulty Progression\n\n| Level | Chapters | Hours |\n|-------|----------|-------|\n| **Foundations** | Ch 0 | 3–4 |\n| **Beginner** | Ch 1–2 | 28–36 |\n| **Intermediate** | Ch 3–8 | 76–96 |\n| **Advanced** | Ch 9–13 | 48–58 |\n| **Expert Distributed** | Ch 14–20 | 98–124 |\n\n---\n\n# INTERVIEW PREPARATION\n\n## Question Coverage by Chapter\n\n| Chapter | Key Interview Questions |\n|---------|------------------------|\n| 0 | Concurrency vs parallelism, GMP model, CSP |\n| 1 | Race conditions, channel vs mutex, select behavior, sync.Once |\n| 2 | Deadlock (Coffman), livelock vs deadlock, starvation detection |\n| 3 | Goroutine leaks, close propagation, fan-in/fan-out |\n| 4 | Leak patterns, pprof, context cancellation |\n| 5 | WaitGroup bugs, errgroup, error aggregation |\n| 6 | Select semantics, time.After leak, priority select |\n| 7 | Thread-safe LRU, RWMutex vs Mutex, lock striping |\n| 8 | sync.Once/Pool/Cond/Map — when and why each |\n| 9 | Graceful shutdown, context propagation |\n| 10 | Rate limiter, circuit breaker, singleflight |\n| 11 | Memory model, happens-before |\n| 12 | pprof, false sharing, Amdahl's Law |\n| 13 | Debugging concurrent code, testing strategies |\n| 14–19 | Distributed patterns (saga, locks, queues, tracing) |\n\n## Question Levels\n\n**L3–L4 (Junior–Mid)**:\n- Fix race conditions\n- Implement worker pools\n- Use channels + select correctly\n- Basic mutex patterns\n- sync.Once for lazy init\n\n**L5–L6 (Senior)**:\n- Design concurrent systems\n- Explain select random selection (extremely common)\n- Detect and fix goroutine leaks\n- Choose right sync primitives\n- Debug production issues\n- sync.Pool for GC pressure\n\n**L7+ (Staff+)**:\n- Distributed systems design\n- Saga patterns\n- Distributed locking\n- Message queue architecture\n- Self-healing systems\n- Livelock/starvation in production\n\n---\n\n# LEARNING OBJECTIVES\n\n## By End of Part 1 (Single-Process)\n\n- ✅ Explain the difference between concurrency and parallelism\n- ✅ Describe Go's GMP scheduler model\n- ✅ Write thread-safe Go code with correct primitive choice\n- ✅ Identify and fix race conditions, deadlocks, livelocks, starvation\n- ✅ Use select correctly (including random selection, nil channels, timeouts)\n- ✅ Use the full sync package (Once, Pool, Cond, Map)\n- ✅ Debug concurrent programs with pprof and race detector\n- ✅ Test concurrent code reliably\n- ✅ Pass L4–L6 concurrency interviews\n\n## By End of Part 2 (Distributed)\n\n- ✅ Coordinate across multiple services\n- ✅ Implement distributed patterns (saga, locks, queues)\n- ✅ Design distributed systems\n- ✅ Build self-healing, observable microservices\n- ✅ Pass Staff+ system design interviews\n\n---\n\n# HOW TO USE THIS COURSE\n\n## Sequential Path (Recommended)\n\n1. Chapter 0 → Chapter 1 → ... → Chapter 20\n2. Do every exercise — the solutions are as important as the problems\n3. One chapter per week is sustainable\n\n## Accelerated Path\n\n1. Ch 0–2 (Foundations + Race + Deadlock/Livelock/Starvation): 3 weeks\n2. Ch 3–8 (Channels, Leaks, WaitGroup, Select, Mutex, Sync Toolbox): 6 weeks\n3. Ch 9–13 (Advanced single-process): 3 weeks\n4. Ch 14–20 (Distributed + Capstone): 6 weeks\n**Total**: 18 weeks at 15–20 hours/week\n\n## Interview Prep Path\n\n**For L4–L5 roles** (8 weeks):\n- Ch 0–9 + practice problems\n- Focus especially on Chapters 1, 2, 6, and 8 (highest interview frequency)\n\n**For Staff+ roles** (12 weeks):\n- Full course Ch 0–20\n- All capstone projects\n- System design practice drawing on Ch 14–19\n\n---\n\n# WHAT'S DIFFERENT FROM OTHER COURSES\n\n| Feature | This Course v3.0 | Best Alternatives |\n|---------|----------------|-------------------|\n| Approach | Problem-first throughout | Theory-first (Educative, Coursera) |\n| Livelock/Starvation | Full chapter treatment, production bugs | Brief mention or skipped |\n| Select statement | Two full treatments (intro + deep-dive) | One shallow pass |\n| sync toolbox (Once/Pool/Cond/Map) | Dedicated chapter, problem-first | Scattered or skipped |\n| Concurrency theory | Problem-first Chapter 0 | Lecture-style or skipped |\n| Testing concurrent code | Dedicated section | Rarely covered |\n| Heartbeat / healing patterns | Chapter 19 | Cox-Buday book only |\n| Confinement pattern | Chapter 10 | Cox-Buday book only |\n| Code | 100% runnable | Often pseudocode |\n| Practice problems | 318+ with full solutions | Few or quiz-only |\n| Time estimate | Realistic (250–300 hrs) | \"Learn in a weekend\" |";
var __STRUCTURE__ = {"title":"Go Concurrency Mastery","overview":[{"id":"COMPLETE_COURSE_CURRICULUM","title":"Go Concurrency Mastery: Complete Course Curriculum","filename":"COMPLETE_COURSE_CURRICULUM.md"}],"chapters":[{"id":"chapter-01","title":"Chapter 01: The Race Condition Crisis","dir":"chapter-01","parts":[{"id":"PART0_INTRODUCTION","title":"Chapter 1, Part 0: Introduction and Setup","filename":"PART0_INTRODUCTION.md"},{"id":"PART1_SEQUENTIAL_BASELINE","title":"Chapter 1, Part 1: The Sequential Baseline","filename":"PART1_SEQUENTIAL_BASELINE.md"},{"id":"PART3_RACE_CONDITIONS_DEEP_DIVE","title":"Chapter 1, Part 3: What IS a Race Condition? - Deep Dive","filename":"PART3_RACE_CONDITIONS_DEEP_DIVE.md"},{"id":"PART4_SOLUTION_CHANNELS","title":"Chapter 1, Part 4: Solution #1 - Channels","filename":"PART4_SOLUTION_CHANNELS.md"},{"id":"PART4-1CHANNELS_PRACTICE","title":"Channels Practice","filename":"PART4-1CHANNELS_PRACTICE.md"}]}]};
</script>
<script src="./app.js"></script>
</body>
</html>